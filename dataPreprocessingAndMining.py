# -*- coding: utf-8 -*-
"""DataCleaning_DMDW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q40kQwpIAW6JTeX7jYDUXZhAP_rnGZSC
"""

import numpy as np
import pandas as pd

cd /content/drive/My Drive/Colab Notebooks/dmdw

#takes a few seconds to run
''' 
Locationdf is a dataframe used to store countries with their latitude and longitude.
Source : https://developers.google.com/public-data/docs/canonical/countries_csv
''' 
locationdf = pd.read_csv('country_centroid.csv')
locationdf = locationdf.sort_values(by='name')
locationdf=locationdf.rename(columns={"name": "Country name"})

'''
happydf is the world happiness index represented as a dataframe
Source : https://www.kaggle.com/unsdsn/world-happiness
'''
happydf = pd.read_csv('2020.csv')
happydf.sort_values(by='Country name')

'''
mergedf is the merge of the two as we want the country centroids
'''
mergedf = pd.merge(locationdf, happydf)
mergedf = mergedf.rename(columns={"country": "iso2"})

'''
citiesdf is the collection of the major cities of each country with atleast a significant population
Source : https://simplemaps.com/data/world-cities
'''
citiesdf=pd.read_csv("worldcities.csv")
citiesdf = citiesdf[['city_ascii','lat','lng','country','iso2','population']]
citiesdf=citiesdf.sort_values(['country','population'], ascending=[True,False])

'''
citiesmergedf is a dataframe which is the merge of the mergedf and the cities dataframe as we
want the centroid of each city for controlled points where we are sure that we can fetch tweets
as the country centroid may not have alot of population eg - russia, australia,etc.
We are taking maximum 5 cities per country ordered by their population for the latitude and longitude
'''
citiesmergedf = pd.merge(mergedf,citiesdf)
citiesmergedf=citiesmergedf.dropna()
citiesmergedf = citiesmergedf.drop(['Regional indicator',	'Ladder score','Standard error of ladder score',	'upperwhisker','lowerwhisker', 'Logged GDP per capita','Social support','Healthy life expectancy',	'Freedom to make life choices','Generosity','Perceptions of corruption','Ladder score in Dystopia','Explained by: Log GDP per capita','Explained by: Social support','Explained by: Healthy life expectancy','Explained by: Freedom to make life choices','Explained by: Generosity','Explained by: Perceptions of corruption','Dystopia + residual'], axis=1)
citiesmergedf = citiesmergedf.groupby('country').head(5)
citiesmergedf = citiesmergedf.reset_index()
citiesmergedf = citiesmergedf.drop(['index'],axis=1)

repeater = ['Azerbaijan',
 'Belarus',
 'Bolivia',
 'Burkina Faso',
 'Burkina Faso',
 'Cameroon',
 'Central African Republic',
 'Central African Republic',
 'Chad',
 'Chad',
 'Croatia']

import itertools
for index, row in citiesmergedf.iterrows():
  if(index<20):
    continue
  latitude = row['lat']    # geographical centre of search
  longitude = row['lng']    # geographical centre of search
  max_range = 20             # search range in kilometres
  num_results = 100        # minimum results to obtain
  outfile = 'lang_output'+row['Country name']+'.csv'
  print(str(index) +" "+ str(row['Country name']))

#!/usr/bin/env python

#-----------------------------------------------------------------------
# twitter-search-geo
#  - performs a search for tweets close to New Cross, London,
#    and outputs them to a CSV file.
#-----------------------------------------------------------------------

from twitter import *

import sys
import csv
import time
consumer_key = 'T7eP9f6XoHjYS6qITDlRA5VTc'

consumer_secret = 'LjXdLvvYc0SdYDJGuOnaLuS7zRK4vIyHhRWBrXrUKh1TShV4aj'

access_key = '1034296635243737088-kFHM3Hi5szDKMdhWdQg4GuDP0ca4HZ'

access_secret = '2ZEwElzsVvS4M7Eh5aeNYYQlhwUCzmFcYsTRQcYpo06KO'

for index, row in citiesmergedf.iterrows():
  if(index%100 == 0):
     time.sleep(1000)
  latitude = row['lat']    # geographical centre of search
  longitude = row['lng']    # geographical centre of search
  max_range = 50             # search range in kilometres
  num_results = 100        # minimum results to obtain
  outfile = 'lang_output'+row['Country name']+'.csv' #output file
  country = row['Country name'] 

  #-----------------------------------------------------------------------
  # load our API credentials
  #-----------------------------------------------------------------------
  import sys
  sys.path.append(".")
  #import config

  #-----------------------------------------------------------------------
  # create twitter API object
  #-----------------------------------------------------------------------
  twitter = Twitter(auth = OAuth(access_key,
                    access_secret,
                    consumer_key,
                    consumer_secret))

  #-----------------------------------------------------------------------
  # open a file to write (mode "w"), and create a CSV writer object
  #-----------------------------------------------------------------------
  # csvfile = open(outfile, "a")
  # csvwriter = csv.writer(csvfile)

  

  
  #-----------------------------------------------------------------------
  # add headings to our CSV file
  #-----------------------------------------------------------------------
  # with open(outfile, "r") as f:
  #   reader = csv.reader(f)
  #   for header in reader:
  #       break
  #row = [ "user", "text", "country", "id" ]
  #csvwriter.writerow(row)

  #-----------------------------------------------------------------------
  # the twitter API only allows us to query up to 100 tweets at a time.
  # to search for more, we will break our search up into 10 "pages", each
  # of which will include 100 matching tweets.
  #-----------------------------------------------------------------------
  result_count = 0
  last_id = None
  fetch_count=0
  while result_count <  num_results:
      #-----------------------------------------------------------------------
      # perform a search based on latitude and longitude
      # twitter API docs: https://dev.twitter.com/rest/reference/get/search/tweets
      #-----------------------------------------------------------------------
      query = twitter.search.tweets(q = "", geocode = "%f,%f,%dkm" % (latitude, longitude, max_range),lang='en', count = 100, max_id = last_id)

      for result in query["statuses"]:
          # if result["geo"]:
          user = result["user"]["screen_name"]
          text = result["text"]
          text = text.encode('ascii', 'replace')
          #print(text)
          id = result["id"]
              # latitude = result["geo"]["coordinates"][0]
              # longitude = result["geo"]["coordinates"][1]

              #-----------------------------------------------------------------------
              # now write this row to our CSV file
              #-----------------------------------------------------------------------
          row = [ user, text, country, id ]#, latitude, longitude ]
          with open(outfile, "a", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(row)
          #csvwriter.writerow(row)
          result_count += 1
          last_id = result["id"]

      #-----------------------------------------------------------------------
      # let the user know where we're up to
      #-----------------------------------------------------------------------
      print("got %d results" % result_count)
      fetch_count+=1
      if(fetch_count>5): #number of repeats for a new tweet that it needs to do per request
        repeater.append(country)
        break

  #-----------------------------------------------------------------------
  # we're all finished, clean up and go home.
  #-----------------------------------------------------------------------
  #csvfile.close()

  print("written to %s" % outfile)

repeater

